(fil <- basename(URL))
if (!file.exists(fil)) download.file(URL, fil)
unzip(fil)
?readOGR
# read in the shapefile and reduce the polygon complexity
dr <- readOGR(sprintf("USDM_%s.shp", week),
sprintf("USDM_%s", week),
verbose=FALSE,
stringsAsFactors=FALSE)
install.packages('lifespan')
devtools::install_github("DataWookie/lifespan")
devtools::install_github("DataWookie/lifespan")
library(lifespan)
library(ggplot2)
ggplot(deathsage, aes(x = year, y = age)) +
+   geom_raster(aes(fill = count)) +
+   labs(x = "Year", y = "Age") +
+   scale_y_continuous(breaks = seq(0, 120, 10), limits = c(0, 110)) +
+   scale_fill_gradient("Deaths", low = "#FFFFFF") +
+   facet_wrap(~ sex) +
+   theme_minimal() +
+   theme(panel.grid = element_blank())
library(lifespan)
library(ggplot2)
ggplot(deathsage, aes(x = year, y = age)) +
geom_raster(aes(fill = count)) +
labs(x = "Year", y = "Age") +
scale_y_continuous(breaks = seq(0, 120, 10), limits = c(0, 110)) +
scale_fill_gradient("Deaths", low = "#FFFFFF") +
facet_wrap(~ sex) +
theme_minimal() +
theme(panel.grid = element_blank())
head(deathsage)
install.packages('mvtboost')
library(mvtboost)
data("mpg",package="ggplot2")
Y <- mpg[,c("cty","hwy")]
Y
str(Y)
library(dplyr)
glimpse(Y)
install.packages('BayesVarSel')
library(BayesVarSel)
data(Ozone35)
data(Ozone35)
View(Ozone35)
dim(Ozone35)
Oz35.GibbsBvs<- GibbsBvs(formula = "y~.", data = Ozone35, prior.betas = "gZellner",
prior.models = "Constant", n.iter = 10000, init.model = "Full", n.burnin = 100,
time.test = FALSE)
Oz35.GibbsBvs
Oz35.GibbsBvs
summary(Oz35.GibbsBvs)
plotBvs(Oz35.GibbsBvs, option="conditional")
data(UScrime)
data(UScrime)
crime.Bvs<- Bvs(formula="y~.", data=UScrime, n.keep=1000)
plotBvs(crime.Bvs, option="dimension")
plotBvs(crime.Bvs, option="joint")
plotBvs(crime.Bvs, option="conditional")
plotBvs(crime.Bvs, option="not")
GibbsBvs
52+15+9+9+20+17+1
2+25+36+32+23+75+19+29
((3+10)/(5-1+20)) * ((20.1)/((123+241)+(123+241)*0.1))
p1 <- ((3+10)/(5-1+20)) * ((20.1)/(123+(123+241)*0.1))
p2 <- ((11)/(5-1+20)) * ((36.1)/(241+(123+241)*0.1))
p1/(p1+p2)
123+241
p2 <- ((11)/(5-1+20)) * ((36.1)/(241+(365)*0.1))
p1 <- ((3+10)/(5-1+20)) * ((20.1)/(123+(365)*0.1))
p1/(p1+p2)
p1 <- ((3+10)/(5-1+20)) * ((20.1)/(123+(10)*0.1))
p2 <- ((11)/(5-1+20)) * ((36.1)/(241+(10)*0.1))
p1/(p1+p2)
install.packages('clusterfly')
library(clusterfly)
install.packages('clustervarsel')
install.packages('clustvarsel')
install.packages('HiddenMarkov')
demo("beta", package="HiddenMarkov")
library(HiddenMarkov)
demo("beta", package="HiddenMarkov")
demo("gamma", package="HiddenMarkov")
install.packages('xgboost')
library(xgboost)
data(agaricus.train, package='xgboost')
#Both dataset are list with two items, a sparse matrix and labels
#(labels = outcome column which will be learned).
#Each column of the sparse Matrix is a feature in one hot encoding format.
train <- agaricus.train
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
xgb.plot.tree(agaricus.train$data@Dimnames[[2]], model = bst)
install.packages('DiagrammeR')
library(DiagrammeR)
xgb.plot.tree(agaricus.train$data@Dimnames[[2]], model = bst)
rm(list = ls())
library(xgboost)
data(agaricus.train, package='xgboost')
#Both dataset are list with two items, a sparse matrix and labels
#(labels = outcome column which will be learned).
#Each column of the sparse Matrix is a feature in one hot encoding format.
train <- agaricus.train
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
#agaricus.test$data@Dimnames[[2]] represents the column names of the sparse matrix.
xgb.plot.tree(agaricus.train$data@Dimnames[[2]], model = bst)
?xgb.plot.multi.trees
xgb.plot.multi.trees(agaricus.train$data@Dimnames[[2]], model = bst)
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
install.packages("xgboost", repos = "http://dmlc.ml/drat/", type = "source")
library(xgboost)
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
xgb.plot.tree(agaricus.train$data@Dimnames[[2]], model = bst)
?xgb.plot.multi.trees
??xgb.plot.multi.trees
xgb.plot.tree(model = bst)
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")
install.packages("xgboost", repos = "http://dmlc.ml/drat/", type = "source")
install.packages("xgboost", repos = "http://dmlc.ml/drat/", type = "source")
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
model <- xgboost(data = train$data, label = train$label,
nrounds = 2, objective = "binary:logistic")
preds = predict(model, test$data)
cv.res <- xgb.cv(data = train$data, label = train$label, nfold = 5,
nrounds = 2, objective = "binary:logistic")
loglossobj <- function(preds, dtrain) {
# dtrain is the internal format of the training data
# We extract the labels from the training data
labels <- getinfo(dtrain, "label")
# We compute the 1st and 2nd gradient, as grad and hess
preds <- 1/(1 + exp(-preds))
grad <- preds - labels
hess <- preds * (1 - preds)
# Return the result as a list
return(list(grad = grad, hess = hess))
}
model <- xgboost(data = train$data, label = train$label,
nrounds = 2, objective = loglossobj, eval_metric = "error")
bst <- xgb.cv(data = train$data, label = train$label, nfold = 5,
nrounds = 20, objective = "binary:logistic",
early.stop.round = 3, maximize = FALSE)
dtrain <- xgb.DMatrix(train$data, label = train$label)
model <- xgboost(data = dtrain, nrounds = 2, objective = "binary:logistic")
pred_train <- predict(model, dtrain, outputmargin=TRUE)
setinfo(dtrain, "base_margin", pred_train)
model <- xgboost(data = dtrain, nrounds = 2, objective = "binary:logistic")
dat <- matrix(rnorm(128), 64, 2)
label <- sample(0:1, nrow(dat), replace = TRUE)
for (i in 1:nrow(dat)) {
ind <- sample(2, 1)
dat[i, ind] <- NA
}
model <- xgboost(data = dat, label = label, missing = NA,
nrounds = 2, objective = "binary:logistic")
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 10, objective = "binary:logistic")
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
bst <- xgboost(data = train$data, label = train$label, max.depth = 15,
eta = 1, nthread = 2, nround = 30, objective = "binary:logistic",
min_child_weight = 50)
xgb.plot.multi.trees(model = bst, feature_names = agaricus.train$data@Dimnames[[2]], features.keep = 3)
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
importance_matrix <- xgb.importance(agaricus.train$data@Dimnames[[2]], model = bst)
xgb.plot.importance(importance_matrix)
install.packages('Ckmeans.1d.dp')
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
importance_matrix <- xgb.importance(agaricus.train$data@Dimnames[[2]], model = bst)
xgb.plot.importance(importance_matrix)
#' Project all trees on one tree and plot it
#'
#' Visualization of the ensemble of trees as a single collective unit.
#'
#' @param model dump generated by the \code{xgb.train} function.
#' @param feature_names names of each feature as a \code{character} vector. Can be extracted from a sparse matrix (see example). If model dump already contains feature names, this argument should be \code{NULL}.
#' @param features_keep number of features to keep in each position of the multi trees.
#' @param plot_width width in pixels of the graph to produce
#' @param plot_height height in pixels of the graph to produce
#' @param ... currently not used
#'
#' @return Two graphs showing the distribution of the model deepness.
#'
#' @details
#'
#' This function tries to capture the complexity of gradient boosted tree ensemble
#' in a cohesive way.
#'
#' The goal is to improve the interpretability of the model generally seen as black box.
#' The function is dedicated to boosting applied to decision trees only.
#'
#' The purpose is to move from an ensemble of trees to a single tree only.
#'
#' It takes advantage of the fact that the shape of a binary tree is only defined by
#' its deepness (therefore in a boosting model, all trees have the same shape).
#'
#' Moreover, the trees tend to reuse the same features.
#'
#' The function will project each tree on one, and keep for each position the
#' \code{features_keep} first features (based on Gain per feature measure).
#'
#' This function is inspired by this blog post:
#' \url{https://wellecks.wordpress.com/2015/02/21/peering-into-the-black-box-visualizing-lambdamart/}
#'
#' @examples
#' data(agaricus.train, package='xgboost')
#'
#' bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 15,
#'                  eta = 1, nthread = 2, nrounds = 30, objective = "binary:logistic",
#'                  min_child_weight = 50)
#'
#' p <- xgb.plot.multi.trees(model = bst, feature_names = colnames(agaricus.train$data), features_keep = 3)
#' print(p)
#'
#' @export
xgb.plot.multi.trees <- function(model, feature_names = NULL, features_keep = 5, plot_width = NULL, plot_height = NULL, ...){
check.deprecation(...)
tree.matrix <- xgb.model.dt.tree(feature_names = feature_names, model = model)
# first number of the path represents the tree, then the following numbers are related to the path to follow
# root init
root.nodes <- tree.matrix[stri_detect_regex(ID, "\\d+-0"), ID]
tree.matrix[ID %in% root.nodes, abs.node.position:=root.nodes]
precedent.nodes <- root.nodes
while(tree.matrix[,sum(is.na(abs.node.position))] > 0) {
yes.row.nodes <- tree.matrix[abs.node.position %in% precedent.nodes & !is.na(Yes)]
no.row.nodes <- tree.matrix[abs.node.position %in% precedent.nodes & !is.na(No)]
yes.nodes.abs.pos <- yes.row.nodes[, abs.node.position] %>% paste0("_0")
no.nodes.abs.pos <- no.row.nodes[, abs.node.position] %>% paste0("_1")
tree.matrix[ID %in% yes.row.nodes[, Yes], abs.node.position := yes.nodes.abs.pos]
tree.matrix[ID %in% no.row.nodes[, No], abs.node.position := no.nodes.abs.pos]
precedent.nodes <- c(yes.nodes.abs.pos, no.nodes.abs.pos)
}
tree.matrix[!is.na(Yes),Yes:= paste0(abs.node.position, "_0")]
tree.matrix[!is.na(No),No:= paste0(abs.node.position, "_1")]
remove.tree <- . %>% stri_replace_first_regex(pattern = "^\\d+-", replacement = "")
tree.matrix[,`:=`(abs.node.position=remove.tree(abs.node.position), Yes=remove.tree(Yes), No=remove.tree(No))]
nodes.dt <- tree.matrix[,.(Quality = sum(Quality)),by = .(abs.node.position, Feature)][,.(Text =paste0(Feature[1:min(length(Feature), features_keep)], " (", Quality[1:min(length(Quality), features_keep)], ")") %>% paste0(collapse = "\n")), by=abs.node.position]
edges.dt <- tree.matrix[Feature != "Leaf",.(abs.node.position, Yes)] %>% list(tree.matrix[Feature != "Leaf",.(abs.node.position, No)]) %>% rbindlist() %>% setnames(c("From", "To")) %>% .[,.N,.(From, To)] %>% .[,N:=NULL]
nodes <- DiagrammeR::create_nodes(nodes = nodes.dt[,abs.node.position],
label = nodes.dt[,Text],
style = "filled",
color = "DimGray",
fillcolor= "Beige",
shape = "oval",
fontname = "Helvetica"
)
edges <- DiagrammeR::create_edges(from = edges.dt[,From],
to = edges.dt[,To],
color = "DimGray",
arrowsize = "1.5",
arrowhead = "vee",
fontname = "Helvetica",
rel = "leading_to")
graph <- DiagrammeR::create_graph(nodes_df = nodes,
edges_df = edges,
graph_attrs = "rankdir = LR")
DiagrammeR::render_graph(graph, width = plot_width, height = plot_height)
}
globalVariables(
c(
".N", "N", "From", "To", "Text", "Feature", "no.nodes.abs.pos", "ID", "Yes", "No", "Tree", "yes.nodes.abs.pos", "abs.node.position"
)
)
bst <- xgboost(data = train$data, label = train$label, max.depth = 15,
eta = 1, nthread = 2, nround = 30, objective = "binary:logistic",
min_child_weight = 50)
xgb.plot.multi.trees(model = bst, feature_names = agaricus.train$data@Dimnames[[2]], features.keep = 3)
library(DiagrammeR)
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
devtools::install_github('rich-iannone/DiagrammeR')
devtools::install_github('rich-iannone/DiagrammeR')
library(devtools)
library(tidyr)
install.packages('devtools')
library(devtools)
devtools::install_github('rich-iannone/DiagrammeR')
library(DiagrammeR)
library(xgboost)
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")
install.packages('xgboost')
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
library(DiagrammeR)
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
xgb.plot.tree(feature_names = agaricus.train$data@Dimnames[[2]], model = bst)
rm(list = ls())
install.packages('depmixS4')
install.packages('TTR')
## Bull and Bear Markets ##
# Load S&P 500 returns
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
sp500 <- getSymbols("AAPL", from="1950-01-01")
library(depmixS4)
library(TTR)
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
# Preprocessing
ep <- endpoints(sp500, on = "months", k = 1)
install.packages('rts')
library(rts)
ep <- endpoints(sp500, on = "months", k = 1)
ep
sp500
sp500 <- sp500[ep[2:(length(ep)-1)]]
sp500
sp500$logret <- log(sp500$Close) - lag(log(sp500$Close))
sp500 <- na.exclude(sp500)
plot(sp500$logret, main = "S&P 500 log Returns")
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
summary(fm2, which = "prior")
summary(fm2, which = "transition")
summary(fm2, which = "response")
tsp500 <- as.ts(sp500)
pbear <- as.ts(posterior(fm2)[, 2])
pbear
tsp(pbear)
tsp(tsp500)
tsp(pbear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], pbear),
main = "Posterior Probability of State=1 (Volatile, Bear Market)")
posterior(fm2)
map.bear <- as.ts(posterior(fm2)[, 1] == 1)
tsp(map.bear) <- tsp(tsp500)
plot(cbind(tsp500[, 6], map.bear),
main = "Maximum A Posteriori (MAP) State Sequence")
summary(fm2)
logret_train <- logret[1:(length(logret) - 3)]
logret_train <- sp500$logret[1:(length(sp500$logret) - 3)]
logret_train
mod <- depmix(logret_train ~ 1, family = gaussian(), nstates = 4, data = sp500)
mod <- depmix(logret_train ~ 1, family = gaussian(), nstates = 4)
sp500
sp500_train <- sp500[1:(length(sp500$logret) - 3), ]
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500)
set.seed(1)
fm_train <- fit(mod, verbose = FALSE)
summary(fm_train)
posterior(fm_train)
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500_train)
set.seed(1)
fm_train <- fit(mod, verbose = FALSE)
summary(fm_train)
tail(posterior(fm_train), 10)
# [-2 наблюдения]
sp500_train <- sp500[1:(length(sp500$logret) - 2), ]
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500_train)
set.seed(1)
fm_train <- fit(mod, verbose = FALSE)
summary(fm_train)
tail(posterior(fm_train), 10)
posterior(fm_train)[dim(posterior(fm_train))[1],]
as.vector(posterior(fm_train)[dim(posterior(fm_train))[1],])
as.vector(posterior(fm_train)[dim(posterior(fm_train))[1],]) *
summary(fm_train, which = "transition")
summary(fm_train, which = "transition")
summary(fm_train, which = "transition")[1, ]
posterior(fm_train)[dim(posterior(fm_train))[1],]$S1
summary(fm_train, which = "transition")
posterior(fm_train)[dim(posterior(fm_train))[1],]$S1 * 0.47
posterior(fm_train)[dim(posterior(fm_train))[1],]$S3 * 0.53
# backward testing hmm
# [-3 наблюдения]
sp500_train <- sp500[1:(length(sp500$logret) - 3), ]
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500_train)
set.seed(1)
fm_train <- fit(mod, verbose = FALSE)
summary(fm_train)
tail(posterior(fm_train), 10)
posterior(fm_train)[dim(posterior(fm_train))[1],]$S1 * 0.473
posterior(fm_train)[dim(posterior(fm_train))[1],]$S3 * 0.405
6.840919e-04 * 0.473 / 0.7628614156
devtools::install_github("DataWookie/feedeR")
library(feereR)
library(feedeR)
?feed.extract
data <- feed.extract("http://www.forecast.ru/feed/rss.xml")
data <- feed.extract("http://static.feed.rbc.ru/rbc/logical/footer/news.rss")
install.packages('roxygen2')
library(roxygen2)
data <- feed.extract("http://static.feed.rbc.ru/rbc/logical/footer/news.rss")
library(devtools)
library(roxygen2)
library(feedeR)
data <- feed.extract("http://static.feed.rbc.ru/rbc/logical/footer/news.rss")
devtools::install_github("DataWookie/feedeR")
devtools::install_github("DataWookie/feedeR", force = TRUE)
library(feedeR)
feed.extract("https://feeds.feedburner.com/RBloggers")
library(roxygen2)
feed.extract("https://feeds.feedburner.com/RBloggers")
install.packages("roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
devtools::install_github("hadley/devtools")
install.packages('git2r')
install.packages("git2r")
devtools::install_github("hadley/devtools")
devtools::install_github("hadley/devtools")
install.packages('devtools')
devtools::install_github("hadley/devtools")
devtools::install_github("klutometis/roxygen")
devtools::install_github("klutometis/roxygen")
install.packages('roxygen2')
library(roxygen2)
library(feedeR)
feed.extract("https://feeds.feedburner.com/RBloggers")
library(digest)
feed.extract("https://feeds.feedburner.com/RBloggers")
feed.extract("http://www.vedomosti.ru/rss/issue")
feed.extract("http://www.vedomosti.ru/rss/issue", "utf8")
data <- feed.extract("http://www.vedomosti.ru/rss/issue", "cp1251")
data$title
iconv(feed.extract("http://www.vedomosti.ru/rss/issue", "utf8")$title, "cp1251")
iconv(feed.extract("http://www.vedomosti.ru/rss/issue", "utf8")$title, from = "utf8", to = "cp1251")
data <- feed.extract("http://static.feed.rbc.ru/rbc/logical/footer/news.rss", "utf8")
data$link
data$updated
data$items
data$items$title
data$items$date
range(data$items$date)
data$items$link
feed.extract
parse.rss
setwd('~/Github/Progs/DataFest2016_R_Py')
library(slidify)
library(slidifyLibraries)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
View(logret_train)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
